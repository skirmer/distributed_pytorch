{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Chapter 1: Distributed Computing\n",
    "\n",
    "\n",
    "When we conceptualize computing, it's easy to limit our thinking to the single machine. However, for many use cases, including machine learning, our needs exceed the limits of one single piece of hardware rapidly. That's when we need to move into distributed computing, or combining multiple machines to create what's called a **cluster**.\n",
    "\n",
    "<img src=\"img/dask-cluster.png\" alt=\"diagram1\" width=\"500\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "This example uses the Dask framework to illustrate how distributed computing works. This kind of architecture is used in many different contexts, including distributed data storage in tools such as Redshift or Elasticsearch, as well as computing in systems like Spark and Dask.\n",
    "\n",
    "The essential concept is that the **user** (you) have one point of interaction with the whole group of computers. You pass instruction to that point, which gets received and handled by a master, **scheduler**, primary, or other sort of brain center for the cluster of machines. This machine will not perform the tasks you ask for, but will distribute the work to other machines.\n",
    "\n",
    "The rest of the cluster is made up of **workers**, or the machines that actually perform the desired work. That might mean they run computations, or they have chunks of your data which they can return, or other activities. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The Machines\n",
    "\n",
    "What are these machines, and where do they come from? There are a number of possibilities. Often today, users will be able to requisition machines from a service like Azure or AWS. Sometimes, businesses will run their own servers at a data center on site and these can provide resources for a cluster. You could even run hardware at home in your closet to serve your cluster needs! \n",
    "\n",
    "Where the machines are housed, and how you get to them, is becoming less and less of a problem as major cloud services make computing resources easy to get at the moment you need them, for only the duration you want. This can reduce the expense and barriers to entry for individuals wanting to experiment and learn about distributed computing.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Types of Machines\n",
    "\n",
    "What does matter more than where you get your cluster machines is what kind of machines you choose. This is an area where the possibilities and options are increasing and can be very overwhelming! Visiting the [AWS list of EC2 Instance Types](https://aws.amazon.com/ec2/instance-types/) gives a laundry list of possible machine options, all with different technical parameters and characteristics. Which one's right for your use case? It's really hard to know.\n",
    "\n",
    "It is, however, important to know the difference broadly between CPU and GPU, so we'll discuss that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### GPU vs CPU\n",
    "\n",
    "At the most basic, a GPU and a CPU are both types of computer processor. \n",
    "\n",
    "<img src=\"img/gpu-v-cpu.png\" alt=\"diagram2\" width=\"600\"/>\n",
    "\n",
    "The internal architecture of each kind of chip is quite different, and as a result, each type has different strengths when it comes to processing information and computation instructions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Cores vs Threads\n",
    "\n",
    "To understand this different architecture, it helps to talk about cores and threads. There is a lot of terminology that is very confusing and sometimes contradictory or overlapping in this space.\n",
    "\n",
    "A **core** is a piece of hardware, while a **thread** is a unit of work. \n",
    "\n",
    "A core receives instructions to process a thread, and returns the results of the work after executing. When we talk about the number of cores a machine or computer has, we mean a concrete piece of hardware. When we talk about the threads, we are discussing how many simultaneous units of work we want to try to run.\n",
    "\n",
    "It's also important to know: A core can really only do one thing at a time. It fakes multitasking by switching back and forth between threads. So when you ask your cluster to have more threads than it has cores, you're asking it to try and multitask in this way.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Architecture\n",
    "\n",
    "Now we can understand what it means to say that a CPU has far fewer cores than a GPU. A large CPU by today's standards could be 64 cores. A large GPU, on the other hand, could have thousands of cores. \n",
    "\n",
    "Why, then, wouldn't we use GPUs for everything? It can do a lot more parallel work because it has so many cores!\n",
    "\n",
    "Well, the GPU has some downsides. It's slower, for some computations, than a CPU. It has less immediate access to memory caches, and it isn't supported by nearly as many programming libraries and frameworks. All of this is because of the inner architecture of a CPU versus a GPU.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "It helps somewhat to look at the internal workings side by side. We won't delve too deeply into these structures for this course, but it's worth a look, and if you're interested there will be links at the end of this chapter for further reading.\n",
    "\n",
    "<img src=\"img/cpu.png\" alt=\"diagram3\" width=\"600\"/>\n",
    "<img src=\"img/gpu-ampere.png\" alt=\"diagram4\" width=\"600\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "As you can see, the hardware of the NVIDIA Ampere GPU, shown at bottom, is dramatically more complex than the architecture of a standard CPU. There are a lot more tools available in the GPU, including elements specifically designed to improve certain graphical performance, but to make code from the CPU run well on a GPU takes substantial work and effort. (There is a glossary at the end of this chapter as well, to help clarify the abbreviations and jargon.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What to Do?\n",
    "\n",
    "Now you might see all this and think, \"oh gosh, this is a lot of work to understand in order to just do my machine learning!\" but don't panic. Knowing the essential facts about the GPU versus the CPU is handy, and understanding the core versus thread is important, but you don't need to know all the workings of the latest GPU chip to be able to get your work done.\n",
    "\n",
    "There are a number of platforms available, like my company, Saturn Cloud, as well as AWS Sagemaker, Domino, Databricks, and more, that will handle the management of a computing cluster and make it easily available for your use. The professionals at these kinds of companies can help advise you about the right choices of machine for your work, and you can also easily try a machine type, see if it works, and close it down if it doesn't. This illustrates one of the biggest advantages of cloud based distributed computing, which is that you can get resources quickly, and stop them just as quickly, without much of any manual effort or labor overhead. \n",
    "\n",
    "For the rest of this course, when I do examples using machine clusters, I'll be using Saturn Cloud to demonstrate, but there are lots of systems that you can try to do your own."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Glossary 1\n",
    "\n",
    "**Warp**  \n",
    "Set of threads based on same code, with same or very similar execution paths. (Single Instruction, Multiple Threads model.) One warp is usually 32 threads.\n",
    "\n",
    "**Warp Scheduler**  \n",
    "A warp scheduler selects a warp and sets it to be executed. If a warp stalls, the scheduler will choose a new warp to execute. \n",
    "\n",
    "**Dispatch Unit**  \n",
    "Receives instructions from Warp Scheduler and passes them to appropriate functional units. \n",
    "\n",
    "**Register File**   \n",
    "Holds specific memory that is accessible to threads. Tends to be big in GPU, because many threads run at once. Enables switching between threads.\n",
    "\n",
    "**vCPU: Virtual CPU**. \n",
    "Instead of thinking of CPUs as hardware, you might measure the power of a CPU, but distribute it across multiple actual pieces of hardware. You can use the computing resources in time slots, sharing with other users. A vCPU represents the computing power of a CPU, across multiple resources.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Glossary 2\n",
    "\n",
    "**Functional Unit**  \n",
    "Any of the types of processing unit on the multiprocessor, including CUDA cores, LD/ST, or SFU.\n",
    "\n",
    "**LD/ST: Load/Store Units**   \n",
    "Loads and stores data from/to memory cache.\n",
    "\n",
    "**SFU: Special Function Units**   \n",
    "Can do more complex mathematics than the CUDA core.\n",
    "\n",
    "**TC: Tensor Cores**   \n",
    "Specific cores designed for tensor calculations, AI, and complex computations.\n",
    "\n",
    "**Texture Unit**  \n",
    "Also known as Texture Mapping Unit or Texture Processing Unit. Enables transformation of flat images to 3D space.\n",
    "\n",
    "**Ray Tracing Core**  \n",
    "Conducts complex geometry calculations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Glossary 3\n",
    "\n",
    "**Operand Collector**  \n",
    "Reads and caches register values from the Register File (found outside the core in Streaming Multiprocessor).\n",
    "\n",
    "**FPU: Floating Point Unit**  \n",
    "Runs floating point arithmetic computations or calculations.\n",
    "\n",
    "**INTU (ALU): Integer Unit**  \n",
    "Runs integer arithmetic computations or calculations. Similar to the ALU in a CPU.\n",
    "\n",
    "**Result Queue**  \n",
    "Writes result values back to Register File (found outside the core in GPU Streaming Multiprocessor).\n",
    "\n",
    "**CU: Control Unit**   \n",
    "Directs the work of the rest of the core, telling the ALU cores what to do.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Reference Links\n",
    "\n",
    "* https://www.nvidia.com/content/dam/en-zz/Solutions/geforce/ampere/pdf/NVIDIA-ampere-GA102-GPU-Architecture-Whitepaper-V1.pdf\n",
    "* https://docs.nvidia.com/cuda/ampere-tuning-guide/index.html\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
